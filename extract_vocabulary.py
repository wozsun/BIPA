#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import glob
import sys
from collections import defaultdict
from datetime import datetime

def extract_chinese_translations(text):
    """
    ä»æ–‡æœ¬ä¸­æå–ä¸­æ–‡ç¿»è¯‘ï¼ŒæŒ‰è‹±æ–‡å®šä¹‰åˆ†ç»„

    å‚æ•°:
        text (str): åŒ…å«ä¸­è‹±æ–‡æ··åˆå†…å®¹çš„æ–‡æœ¬

    è¿”å›:
        list: ç¿»è¯‘ç»„åˆ—è¡¨ï¼Œæ¯ä¸ªç»„åŒ…å«åŒä¸€è‹±æ–‡å®šä¹‰ä¸‹çš„æ‰€æœ‰ä¸­æ–‡ç¿»è¯‘

    å¤„ç†é€»è¾‘:
        - è‹±æ–‡è¡Œä½œä¸ºåˆ†ç»„æ ‡å¿—ï¼Œä¸­æ–‡è¡Œä¸ºç¿»è¯‘å†…å®¹
        - æ”¯æŒä¸­æ–‡é€—å·åˆ†éš”çš„åŒä¹‰è¯
        - æ™ºèƒ½å¤„ç†æ‹¬å·å†…å®¹ï¼Œé¿å…é”™è¯¯åˆ†å‰²
    """
    translation_groups = []  # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªè¯ä¹‰ç»„ï¼ˆå¯¹åº”ä¸€ä¸ªè‹±æ–‡å®šä¹‰ï¼‰
    lines = text.split('\n')
    current_group_translations = []

    for line in lines:
        line = line.strip()

        # å¦‚æœæ˜¯è‹±æ–‡è¡Œï¼Œç»“æŸå½“å‰ç»„å¹¶å¼€å§‹æ–°ç»„
        if line and not re.search(r'[\u4e00-\u9fff]', line):
            if current_group_translations:
                translation_groups.append(current_group_translations)
                current_group_translations = []
            continue

        # å¦‚æœæ˜¯ä¸­æ–‡è¡Œï¼Œæ·»åŠ åˆ°å½“å‰ç»„
        if re.search(r'[\u4e00-\u9fff]', line):
            # æŒ‰ä¸­æ–‡é€—å·åˆ†å‰²åŒä¹‰è¯
            if 'ï¼Œ' in line:
                # åˆ†å‰²æ—¶è¦è€ƒè™‘æ‹¬å·å†…çš„å†…å®¹ä¸è¢«åˆ†å‰²
                parts = []
                current_part = ""
                paren_level = 0

                i = 0
                while i < len(line):
                    char = line[i]
                    if char in 'ï¼ˆ(':
                        paren_level += 1
                        current_part += char
                    elif char in 'ï¼‰)':
                        paren_level -= 1
                        current_part += char
                    elif char == 'ï¼Œ' and paren_level == 0:
                        if current_part.strip():
                            parts.append(current_part.strip())
                        current_part = ""
                    else:
                        current_part += char
                    i += 1

                if current_part.strip():
                    parts.append(current_part.strip())

                current_group_translations.extend(parts)
            else:
                # æ•´è¡Œä½œä¸ºä¸€ä¸ªç¿»è¯‘
                current_group_translations.append(line.strip())

    # æ·»åŠ æœ€åä¸€ä¸ªç»„
    if current_group_translations:
        translation_groups.append(current_group_translations)

    return translation_groups

def add_vocabulary_to_file(file_path, vocabulary):
    """
    å°†ç”Ÿæˆçš„è¯æ±‡è¡¨ç›´æ¥æ’å…¥åˆ°åŸè¯æ±‡æ–‡ä»¶ä¸­

    å‚æ•°:
        file_path (str): ç›®æ ‡æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        vocabulary (dict): è¯æ±‡å­—å…¸ï¼Œé”®ä¸ºå°å°¼è¯­å•è¯ï¼Œå€¼ä¸ºç¿»è¯‘ç»„åˆ—è¡¨

    åŠŸèƒ½è¯´æ˜:
        - åœ¨YAML front matterä¹‹åæ’å…¥è¯æ±‡è¡¨
        - å¦‚æœæ–‡ä»¶å·²å­˜åœ¨è¯æ±‡è¡¨ï¼Œåˆ™æ›¿æ¢ä¸ºæ–°ç‰ˆæœ¬
        - è¯æ±‡è¡¨ä½¿ç”¨åŠ ç²—æ ‡é¢˜å’ŒMarkdownè¡¨æ ¼æ ¼å¼
        - è¯æ±‡æŒ‰å­—æ¯é¡ºåºæ’åº
        - è‡ªåŠ¨å¤„ç†æ–‡ä»¶ç¼–ç å’Œé”™è¯¯æƒ…å†µ
    """
    if not vocabulary:
        return

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()

        # ç”Ÿæˆè¯æ±‡è¡¨å†…å®¹
        vocab_content = "\n**è¯æ±‡è¡¨**\n\n| å°å°¼è¯­ | ä¸­æ–‡ç¿»è¯‘ |\n|--------|----------|\n"

        # æŒ‰å­—æ¯é¡ºåºæ’åº
        for word in sorted(vocabulary.keys()):
            translations = merge_translations([vocabulary[word]])
            vocab_content += f"| {word} | {translations} |\n"

        vocab_content += "\n---\n"

        # æ‰¾åˆ°YAML front matterçš„ç»“æŸä½ç½®
        if original_content.startswith('---\n'):
            end_pos = original_content.find('\n---\n', 4)
            if end_pos != -1:
                # åœ¨YAML front matterä¹‹åæ’å…¥è¯æ±‡è¡¨
                yaml_part = original_content[:end_pos + 5]  # åŒ…å«ç»“æŸçš„---\n
                rest_content = original_content[end_pos + 5:]

                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨è¯æ±‡è¡¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›¿æ¢
                if "**è¯æ±‡è¡¨**" in rest_content:
                    # æ‰¾åˆ°è¯æ±‡è¡¨çš„ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ª---æˆ–æ–‡ä»¶æœ«å°¾ï¼‰
                    vocab_start = rest_content.find("**è¯æ±‡è¡¨**")
                    vocab_end = rest_content.find("\n---\n", vocab_start)
                    if vocab_end == -1:
                        # å¦‚æœæ‰¾ä¸åˆ°ç»“æŸæ ‡è®°ï¼ŒæŸ¥æ‰¾ä¸‹ä¸€ä¸ª#æ ‡é¢˜
                        vocab_end = rest_content.find("\n# ", vocab_start)
                        if vocab_end == -1:
                            vocab_end = len(rest_content)
                    else:
                        vocab_end += 5  # åŒ…å«\n---\n

                    # æ›¿æ¢ç°æœ‰è¯æ±‡è¡¨
                    new_content = yaml_part + vocab_content + rest_content[vocab_end:]
                else:
                    # æ·»åŠ æ–°è¯æ±‡è¡¨
                    new_content = yaml_part + vocab_content + rest_content

                # å†™å›æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)

                print(f"è¯æ±‡è¡¨å·²æ·»åŠ åˆ°æ–‡ä»¶: {file_path}")
    except Exception as e:
        print(f"æ·»åŠ è¯æ±‡è¡¨åˆ°æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def extract_vocabulary_from_file(file_path):
    """
    ä»å•ä¸ªMarkdownè¯æ±‡æ–‡ä»¶ä¸­æå–æ‰€æœ‰è¯æ±‡æ¡ç›®

    å‚æ•°:
        file_path (str): è¯æ±‡æ–‡ä»¶çš„ç»å¯¹è·¯å¾„

    è¿”å›:
        dict: è¯æ±‡å­—å…¸ï¼Œé”®ä¸ºå°å°¼è¯­å•è¯ï¼Œå€¼ä¸ºè¯¥å•è¯çš„æ‰€æœ‰ç¿»è¯‘ç»„

    æ–‡ä»¶æ ¼å¼è¦æ±‚:
        - ä½¿ç”¨"# å•è¯å"æ ¼å¼æ ‡è®°æ¯ä¸ªè¯æ¡
        - è¯æ¡å†…å®¹åŒ…å«ä¸­è‹±æ–‡æ··åˆçš„é‡Šä¹‰
        - æ”¯æŒå¤šè¡Œç¿»è¯‘å’Œè‹±æ–‡å®šä¹‰åˆ†ç»„
        - è‡ªåŠ¨å¿½ç•¥YAML front matterå’Œæ— æ•ˆå†…å®¹
    """
    vocabulary = {}

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except:
        return vocabulary

    # æŒ‰ # åˆ†å‰²å†…å®¹ï¼Œæ¯ä¸ªéƒ¨åˆ†å¯¹åº”ä¸€ä¸ªå•è¯
    sections = content.split('\n# ')

    for section in sections:
        if not section.strip():
            continue

        lines = section.strip().split('\n')
        if not lines:
            continue

        # ç¬¬ä¸€è¡Œæ˜¯å°å°¼è¯­å•è¯ï¼ˆå¯èƒ½åŒ…å«å‰å¯¼çš„#ï¼‰
        indonesian_word = lines[0].replace('#', '').strip()
        if not indonesian_word or indonesian_word.startswith('---'):
            continue

        # æå–ä¸­æ–‡ç¿»è¯‘
        remaining_text = '\n'.join(lines[1:])
        translation_groups = extract_chinese_translations(remaining_text)

        if translation_groups:
            vocabulary[indonesian_word] = translation_groups

    return vocabulary

def merge_translations(translation_groups_list):
    """
    æ™ºèƒ½åˆå¹¶å¤šä¸ªæ¥æºçš„ç¿»è¯‘ç»„ï¼Œå»é™¤é‡å¤å¹¶ä¼˜åŒ–æ ¼å¼

    å‚æ•°:
        translation_groups_list (list): ç¿»è¯‘ç»„åˆ—è¡¨çš„åˆ—è¡¨

    è¿”å›:
        str: åˆå¹¶åçš„ç¿»è¯‘å­—ç¬¦ä¸²ï¼Œç»„å†…ç”¨é€—å·åˆ†éš”ï¼Œç»„é—´ç”¨åˆ†å·åˆ†éš”

    å¤„ç†è§„åˆ™:
        1. å…¨å±€å»é‡ï¼šç§»é™¤å®Œå…¨é‡å¤çš„ç¿»è¯‘
        2. "çš„"å­—ä¼˜åŒ–ï¼šä¿ç•™ä¸å¸¦"çš„"çš„ç‰ˆæœ¬ï¼Œå»é™¤é‡å¤çš„å¸¦"çš„"ç‰ˆæœ¬
        3. ä¿æŒåˆ†ç»„ç»“æ„ï¼šä¸åŒè‹±æ–‡å®šä¹‰çš„ç¿»è¯‘ç”¨åˆ†å·åˆ†éš”
        4. ç»„å†…åŒä¹‰è¯ï¼šç”¨ä¸­æ–‡é€—å·è¿æ¥

    ç¤ºä¾‹:
        è¾“å…¥: [["å¥½çš„", "è‰¯å¥½"], ["å¥½", "ä¸é”™çš„"]]
        è¾“å‡º: "å¥½ï¼Œè‰¯å¥½ï¼›ä¸é”™"
    """
    if not translation_groups_list:
        return ""

    # åˆå¹¶æ‰€æœ‰æ¥æºçš„ç¿»è¯‘ç»„
    all_groups = []
    for groups in translation_groups_list:
        all_groups.extend(groups)

    if not all_groups:
        return ""

    # é¦–å…ˆæ”¶é›†æ‰€æœ‰ç¿»è¯‘è¿›è¡Œå…¨å±€"çš„"å­—å¤„ç†
    all_translations = []
    for group in all_groups:
        if group:
            all_translations.extend(group)

    # å…¨å±€å¤„ç†"çš„"å­—å»é‡
    unique_translations = []
    seen = set()
    for trans in all_translations:
        if trans not in seen:
            unique_translations.append(trans)
            seen.add(trans)

    # å¤„ç†"çš„"å­—å»é‡ï¼ˆå…¨å±€èŒƒå›´ï¼‰
    final_global_translations = []
    used = set()

    for i, trans in enumerate(unique_translations):
        if i in used:
            continue

        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„"çš„"å­—ç‰ˆæœ¬
        de_version = trans + "çš„"
        non_de_version = trans[:-1] if trans.endswith("çš„") else None

        # å¦‚æœå½“å‰è¯ä¸å¸¦"çš„"ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¸¦"çš„"çš„ç‰ˆæœ¬
        if de_version in unique_translations:
            de_index = unique_translations.index(de_version)
            used.add(de_index)
            final_global_translations.append(trans)  # ä¿ç•™ä¸å¸¦"çš„"çš„ç‰ˆæœ¬
        # å¦‚æœå½“å‰è¯å¸¦"çš„"ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸å¸¦"çš„"çš„ç‰ˆæœ¬
        elif non_de_version and non_de_version in unique_translations:
            non_de_index = unique_translations.index(non_de_version)
            if non_de_index not in used:
                used.add(i)  # è·³è¿‡å½“å‰å¸¦"çš„"çš„ç‰ˆæœ¬
                continue
            else:
                final_global_translations.append(trans)
        else:
            final_global_translations.append(trans)

        used.add(i)

    # åˆ›å»ºå…¨å±€å»é‡åçš„ç¿»è¯‘é›†åˆ
    global_unique_set = set(final_global_translations)

    # å¤„ç†æ¯ä¸ªç»„ï¼Œä¿æŒç»„çš„ç»“æ„ä½†åªä¿ç•™å…¨å±€å»é‡åçš„ç¿»è¯‘
    processed_groups = []

    for group in all_groups:
        if not group:
            continue

        # è¿‡æ»¤ç»„å†…ç¿»è¯‘ï¼Œåªä¿ç•™å…¨å±€å»é‡åå­˜åœ¨çš„ç¿»è¯‘
        group_translations = []
        for trans in group:
            if trans in global_unique_set:
                group_translations.append(trans)
                # ä»å…¨å±€é›†åˆä¸­ç§»é™¤ï¼Œé¿å…åœ¨åç»­ç»„ä¸­é‡å¤å‡ºç°
                global_unique_set.discard(trans)

        if group_translations:
            processed_groups.append('ï¼Œ'.join(group_translations))

    # ä½¿ç”¨ä¸­æ–‡åˆ†å·è¿æ¥ä¸åŒçš„ç»„
    return 'ï¼›'.join(processed_groups)

def main():
    # æ£€æŸ¥å¸®åŠ©å‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help', 'help']:
        print("""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              BIPAå°å°¼è¯­è¯æ±‡æå–å·¥å…·
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“š åŠŸèƒ½ç‰¹æ€§ï¼š
   â€¢ è‡ªåŠ¨æå–Markdownè¯æ±‡æ–‡ä»¶ä¸­çš„å°å°¼è¯­-ä¸­æ–‡è¯æ±‡å¯¹
   â€¢ ä¿æŒæŒ‰è‹±æ–‡å®šä¹‰åˆ†ç»„çš„å¤æ‚è¯æ±‡ç»“æ„
   â€¢ æ™ºèƒ½å¤„ç†å¤šä¹‰è¯å¹¶è‡ªåŠ¨å»é™¤é‡å¤ç¿»è¯‘
   â€¢ è‡ªåŠ¨ä¸ºæ‰€æœ‰è¯æ±‡æ–‡ä»¶åµŒå…¥è¯æ±‡è¡¨å¹¶ç”Ÿæˆæ±‡æ€»æ–‡ä»¶

ğŸš€ ä½¿ç”¨æ–¹æ³•ï¼š

1ï¸âƒ£  æ‰¹é‡å¤„ç†æ‰€æœ‰BIPA3è¯æ±‡æ–‡ä»¶ï¼š
   python3 extract_vocabulary.py

   â€¢ æ‰«æBIPA3ç›®å½•ä¸‹æ‰€æœ‰è¯æ±‡æ–‡ä»¶
   â€¢ ç”Ÿæˆæ±‡æ€»è¯æ±‡è¡¨ï¼šBIPA3/Kosakata.md
   â€¢ è‡ªåŠ¨ä¸ºæ‰€æœ‰è¯æ±‡æ–‡ä»¶åµŒå…¥è¯æ±‡è¡¨

ğŸ“„ æ–‡ä»¶æ ¼å¼è¦æ±‚ï¼š
   â€¢ è¯æ±‡æ–‡ä»¶ä½¿ç”¨"# å•è¯å"æ ‡è®°è¯æ¡

ğŸ“Š è¾“å‡ºæ ¼å¼ï¼š
   â€¢ æ±‡æ€»æ–‡ä»¶ï¼šBIPA3ç›®å½•ä¸‹çš„"Kosakata.md"
   â€¢ è‡ªåŠ¨åµŒå…¥ï¼šä¸ºæ‰€æœ‰è¯æ±‡æ–‡ä»¶åµŒå…¥è¯æ±‡è¡¨

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """)
        return

    # æŸ¥æ‰¾æ‰€æœ‰BIPA3ä¸‹çš„è¯æ±‡æ–‡ä»¶
    base_path = "/Users/wozsun/Library/Mobile Documents/iCloud~md~obsidian/Documents/Docs/BIPA/BIPA3"
    vocab_files = glob.glob(os.path.join(base_path, "**/Kosakata/*.md"), recursive=True)

    print(f"æ‰¾åˆ° {len(vocab_files)} ä¸ªè¯æ±‡æ–‡ä»¶")

    if not vocab_files:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯æ±‡æ–‡ä»¶ï¼")
        print(f"æœç´¢è·¯å¾„: {base_path}")
        return

    # åˆå¹¶æ‰€æœ‰è¯æ±‡
    all_vocabulary = defaultdict(list)

    for file_path in vocab_files:
        print(f"å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        vocab = extract_vocabulary_from_file(file_path)

        # ä¸ºæ‰€æœ‰æ–‡ä»¶æ·»åŠ è¯æ±‡è¡¨
        if vocab:
            add_vocabulary_to_file(file_path, vocab)
            print(f"  - å·²æ·»åŠ è¯æ±‡è¡¨åˆ°æ–‡ä»¶å†…")

        for word, translation_groups in vocab.items():
            all_vocabulary[word].extend(translation_groups)

    print(f"æ€»å…±æå–åˆ° {len(all_vocabulary)} ä¸ªå°å°¼è¯­å•è¯")

    # ç”Ÿæˆmarkdownè¡¨æ ¼
    markdown_content = f"""**ç»Ÿè®¡ä¿¡æ¯ï¼š**

- æ€»è¯æ±‡æ•°é‡ï¼š{len(all_vocabulary)} ä¸ª
- æå–æ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}
- å¤„ç†æ–‡ä»¶æ•°ï¼š{len(vocab_files)} ä¸ª

| å°å°¼è¯­ | ä¸­æ–‡ç¿»è¯‘ |
|--------|----------|
"""

    # æŒ‰å­—æ¯é¡ºåºæ’åº
    for word in sorted(all_vocabulary.keys()):
        translations = merge_translations([all_vocabulary[word]])
        markdown_content += f"| {word} | {translations} |\n"

    # ä¿å­˜åˆ°BIPA3ç›®å½•
    output_path = "/Users/wozsun/Library/Mobile Documents/iCloud~md~obsidian/Documents/Docs/BIPA/BIPA3/Kosakata.md"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(markdown_content)

    print(f"è¯æ±‡è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    print(f"å…±åŒ…å« {len(all_vocabulary)} ä¸ªå•è¯")
    print(f"å·²ä¸º {len(vocab_files)} ä¸ªæ–‡ä»¶æ·»åŠ äº†è¯æ±‡è¡¨")

if __name__ == "__main__":
    main()
